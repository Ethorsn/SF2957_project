\section{Summary} \label{sec:summary}
For this project we have implemented a Q-learning algorithm for Blackjack, in the process
enhancing a popular open source implementation of a Blackjack learning environment to
support a richer state space and an arbitrary number of decks. In particular, we have shown 
that an extended state space which stores the number of each type of card the player holds 
leads to a decreased expected loss compared to a state which only stores the sum of the 
values of the cards in the player's hands. In terms of the version of Blackjack played,
further extensions to our work could make the game more realistic by adding features such
as splitting and insurance. An extension for the Q-learning algorithm in this case may be
to run a smaller number of training episodes using the sum-state representation.
When running the algorithm with an extended state, then, we could initialize the Q-function
values for a given (and previously unseen) state by the corresponding values obtained for
the sum-state representation, simply by calculating the sum of the player's hand in the
extended state. This may improve convergence.

Also, the inclusion of several agents in the same training environment would be a very interesting iteration.
The agents would be playing on the same ''table'' and sharing information by viewing their respective cards. In
this context, it would also be interesting if we could introduce a memory to the player, such that it remembers
the cards that have been played. In essence, we would like to see if the algorithm can learn how to count cards. 

As a follow-up on the goals we set for this project, we note that we accomplished all of
them: (1) we implemented a reinforcement learning algorithm (Q-learning) for Blackjack
in (2) Python 3.6 (although we did not see an opportunity to use Tensorflow), and
we have (3) provided a public GitHub repository with our code. However, because getting the 
environment to work properly code-wise, we did not have time to implement our `if time permits'-goals
of implementing more than one reinforcement learning algorithm and use an ensemble method to
combine them.

\textit{
As we saw in section \ref{sec:results} the performance of the Q-learning algorithm was no good on the expanded state space. The average return of the algorithm was always higher when itlearned on the state space using the sum. The algorithm using the state space of the hand was better when 1 deck was used, which is nice but not applicable in real life when blackjack  is played.
Another loss was also the increase in computational time by a factor of 50\%. This is not scalable for large experiments. It could possibly be circumvented by creating more efficient 
algorithms or by using another API instead of ''Gym''.
}
